{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NVIDIA NIMs\n",
    "\n",
    "The `langchain-nvidia-ai-endpoints` package contains LangChain integrations for chat models and embeddings powered by [NVIDIA AI Foundation Models](https://www.nvidia.com/en-us/ai-data-science/foundation-models/), and hosted on the [NVIDIA API Catalog](https://build.nvidia.com/).\n",
    "\n",
    "NVIDIA AI Foundation models are community- and NVIDIA-built models that are optimized to deliver the best performance on NVIDIA-accelerated infrastructure. \n",
    "You can use the API to query live endpoints that are available on the NVIDIA API Catalog to get quick results from a DGX-hosted cloud compute environment, \n",
    "or you can download models from NVIDIA's API catalog with NVIDIA NIM, which is included with the NVIDIA AI Enterprise license. \n",
    "The ability to run models on-premises gives your enterprise ownership of your customizations and full control of your IP and AI application. \n",
    "\n",
    "NIM microservices are packaged as container images on a per model/model family basis \n",
    "and are distributed as NGC container images through the [NVIDIA NGC Catalog](https://catalog.ngc.nvidia.com/). \n",
    "At their core, NIM microservices are containers that provide interactive APIs for running inference on an AI Model. \n",
    "\n",
    "Use this documentation to learn how to install the `langchain-nvidia-ai-endpoints` package and use it to explore the `NVIDIARerank` and `NVIDIAEmbeddings` classes. This notebook demonstrates how you can use a re-ranking model to combine retrieval results and improve accuracy during retrieval of documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install the Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade --quiet langchain-nvidia-ai-endpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Access the NVIDIA API Catalog\n",
    "\n",
    "To get access to the NVIDIA API Catalog, do the following:\n",
    "\n",
    "1. Create a free account on the [NVIDIA API Catalog](https://build.nvidia.com/) and log in.\n",
    "2. Click your profile icon, and then click **API Keys**. The **API Keys** page appears.\n",
    "3. Click **Generate API Key**. The **Generate API Key** window appears.\n",
    "4. Click **Generate Key**. You should see **API Key Granted**, and your key appears.\n",
    "5. Copy and save the key as `NVIDIA_API_KEY`.\n",
    "6. To verify your key, use the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "# del os.environ['NVIDIA_API_KEY']  ## delete key and reset\n",
    "if os.environ.get(\"NVIDIA_API_KEY\", \"\").startswith(\"nvapi-\"):\n",
    "    print(\"Valid NVIDIA_API_KEY already in environment. Delete to reset\")\n",
    "else:\n",
    "    nvapi_key = getpass.getpass(\"NVAPI Key (starts with nvapi-): \")\n",
    "    assert nvapi_key.startswith(\"nvapi-\"), f\"{nvapi_key[:5]}... is not a valid key\"\n",
    "    os.environ[\"NVIDIA_API_KEY\"] = nvapi_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now use your key to access endpoints on the NVIDIA API Catalog."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Work with the API Catalog\n",
    "\n",
    "To test your connection to the API catalog, submit a query to the [nv-embedqa-e5-v5](https://build.nvidia.com/nvidia/nv-embedqa-e5-v5/modelcard) model by running the following code. If you don't specify a model, the embedder uses the default model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA, NVIDIAEmbeddings, NVIDIARerank\n",
    "\n",
    "# embedder.get_available_models()\n",
    "\n",
    "embedder = NVIDIAEmbeddings(model=\"nvidia/nv-embedqa-e5-v5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Truncation\n",
    "\n",
    "Embedding and reranking models typically have a fixed context window that determines the maximum number of input tokens that can be processed. This limit can be a hard limit, equal to the model's maximum input token length, or an effective limit, beyond which the accuracy of the process decreases. Since models operate on tokens, and applications usually work with text, it can be challenging for an application to ensure that its input stays within the model's token limits. By default, an exception is thrown if the input is too large.\n",
    "\n",
    "NVIDIA NIM microservices can truncate the input on the server side if it's too large. The `truncate` parameter accepts the following values:\n",
    "\n",
    "- **NONE** – An exception is thrown if the input is too large. This is the default option.\n",
    "- **START** – The server truncates from the start of the input, discarding tokens as necessary.\n",
    "- **END** – The server truncates from the end of the input, discarding tokens as necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining results from multiple sources\n",
    "\n",
    "Consider a pipeline with data from a BM25 store as well as a semantic store, such as FAISS. Each store is queried independently and returns results that the individual store considers to be highly relevant. Figuring out the overall relevance of the results is where re-ranking comes into play. We will search for information about the query `What is the meaning of life?` across a both a BM25 store and semantic store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is the meaning of life?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### BM25 relevant documents\n",
    "\n",
    "First let's create a BM25 index that we can query. We use the [`BM25Retriever`](hhttps://python.langchain.com/v0.2/docs/integrations/retrievers/bm25/) and web search results from [DuckDuckGo](https://duckduckgo.com/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade --quiet langchain-community duckduckgo-search beautifulsoup4 rank_bm25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade --quiet ddgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.utilities import DuckDuckGoSearchAPIWrapper\n",
    "from langchain_community.retrievers import BM25Retriever\n",
    "from langchain_core.documents import Document\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_documents(query, search_util, text_splitter, source) -> List[Document]:\n",
    "    documents = []\n",
    "    print(f\"Building documents for {query}\")\n",
    "    for result in search_util(query):\n",
    "        print(f\"Processing {result['title']} - {result['link']}\")\n",
    "        try:\n",
    "            text = BeautifulSoup(requests.get(result[\"link\"]).text, \"html.parser\").get_text()\n",
    "            for text in text_splitter.split_text(text):\n",
    "                documents.append(\n",
    "                    Document(\n",
    "                        page_content=text,\n",
    "                        metadata={\n",
    "                            \"title\": result[\"title\"],\n",
    "                            \"url\": result[\"link\"],\n",
    "                            \"source\": source,\n",
    "                        },\n",
    "                    )\n",
    "                )\n",
    "        except Exception as e:\n",
    "            print(f\"Skipping due to connection error: {e}\")\n",
    "    print(f\"Done building {len(documents)} documents\")\n",
    "    return documents        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This might take a few minutes to run.\n",
    "\n",
    "bm25_tool = lambda query: DuckDuckGoSearchAPIWrapper().results(query, max_results=100, source=\"text\")\n",
    "bm25_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200, length_function=len)\n",
    "bm25_retriever = BM25Retriever.from_documents(build_documents(query, bm25_tool, bm25_splitter, \"DuckDuckGo Text\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Return the relevant documents from the query `\"What is the meaning of life?\"` with the BM25 retriever."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm25_retriever.k = 500\n",
    "bm25_docs = bm25_retriever.invoke(query)\n",
    "len(bm25_docs), bm25_docs[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Semantic documents\n",
    "\n",
    "First let's create a FAISS index that we can query. We use web search results from [DuckDuckGo](https://duckduckgo.com/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the following install command that matches your hardware\n",
    "\n",
    "# %pip install --upgrade --quiet faiss-gpu\n",
    "%pip install --upgrade --quiet faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "# De-serialization relies on loading a pickle file.\n",
    "# Pickle files can be modified to deliver a malicious payload that results in execution of arbitrary code on your machine.\n",
    "# Only perform this with a pickle file you have created and no one else has modified.\n",
    "allow_dangerous_deserialization=True\n",
    "\n",
    "sem_tool = lambda query: DuckDuckGoSearchAPIWrapper().results(query, max_results=100, source=\"news\")\n",
    "sem_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100, length_function=len)\n",
    "sem_store = FAISS.from_documents(build_documents(query, sem_tool, sem_splitter, \"DuckDuckGo News\"), embedding=NVIDIAEmbeddings(truncate=\"END\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "sem_retriever = sem_store.as_retriever(\n",
    "    search_kwargs = {\"k\": 500},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Return the relevant documents from the query `\"What is the meaning of life?\"` with FAISS semantic store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sem_docs = sem_retriever.invoke(query)\n",
    "len(sem_docs), sem_docs[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine and rank documents\n",
    "\n",
    "Let's combine the BM25 and semantic search results. The resulting documents are ordered by their relevance to the query by the reranking NIM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranker = NVIDIARerank(truncate=\"END\")\n",
    "\n",
    "all_docs = bm25_docs + sem_docs\n",
    "\n",
    "ranker.top_n = 5\n",
    "docs = ranker.compress_documents(query=query, documents=all_docs)\n",
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self-host with NVIDIA NIM Microservices\n",
    "\n",
    "When you are ready to deploy your AI application, you can self-host models with NVIDIA NIM. \n",
    "For more information, refer to [NVIDIA NIM Microservices](https://www.nvidia.com/en-us/ai-data-science/products/nim-microservices/).\n",
    "\n",
    "The following code connects to locally hosted NIM Microservices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA, NVIDIAEmbeddings, NVIDIARerank\n",
    "\n",
    "# connect to an chat NIM running at localhost:8000, specifying a specific model\n",
    "llm = ChatNVIDIA(base_url=\"http://localhost:8000/v1\", model=\"meta/llama3-8b-instruct\")\n",
    "\n",
    "# connect to an embedding NIM running at localhost:8080\n",
    "embedder = NVIDIAEmbeddings(base_url=\"http://localhost:8080/v1\")\n",
    "\n",
    "# connect to a reranking NIM running at localhost:2016\n",
    "ranker = NVIDIARerank(base_url=\"http://localhost:2016/v1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Related Topics\n",
    "\n",
    "- [langchain-nvidia-ai-endpoints package ReadMe](https://github.com/langchain-ai/langchain-nvidia/blob/main/libs/ai-endpoints/README.md)\n",
    "- [Overview of NVIDIA NIM for Large Language Models (LLMs)](https://docs.nvidia.com/nim/large-language-models/latest/introduction.html)\n",
    "- [Overview of NeMo Retriever Embedding NIM](https://docs.nvidia.com/nim/nemo-retriever/text-embedding/latest/overview.html)\n",
    "- [Overview of NeMo Retriever Reranking NIM](https://docs.nvidia.com/nim/nemo-retriever/text-reranking/latest/overview.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
