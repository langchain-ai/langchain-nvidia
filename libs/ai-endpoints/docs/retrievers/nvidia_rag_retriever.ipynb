{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "234cfa42",
   "metadata": {},
   "source": [
    "# NvidiaRetriever: NVIDIA RAG Blueprint Integration\n",
    "\n",
    "The `NvidiaRetriever` connects LangChain to the [NVIDIA RAG Blueprint](https://github.com/NVIDIA/GenerativeAIExamples/tree/main/RAG) `/v1/search` endpoint. Use it to retrieve relevant documents from a containerized RAG deployment.\n",
    "\n",
    "**Prerequisites:**\n",
    "- A running NVIDIA RAG server (typically at `http://localhost:8081`). Follow the documentation [here](https://docs.nvidia.com/rag/latest/deploy-docker-self-hosted.html) to set it up.\n",
    "- At least one ingested collection in the vector database (e.g. `test_multimodal_query` as shown in this example)\n",
    "\n",
    "**Features:**\n",
    "- Sync and async retrieval\n",
    "- Full support for DocumentSearch parameters (reranker, query rewriting, filters, etc.)\n",
    "- Clear exceptions when the server is unreachable or returns errors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d63503",
   "metadata": {},
   "source": [
    "## Install the Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc38f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade --quiet langchain-nvidia-ai-endpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef98544",
   "metadata": {},
   "source": [
    "## Basic Usage\n",
    "\n",
    "Create a retriever pointing at your RAG server. The default base URL is `http://localhost:8081`. Specify the collection(s) to search and the number of documents to return (`k`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29f3788",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_nvidia_ai_endpoints import NvidiaRetriever\n",
    "\n",
    "retriever = NvidiaRetriever(\n",
    "    base_url=\"http://localhost:8081\",\n",
    "    collection_names=[\"test_multimodal_query\"],  # Provide your collection names here\n",
    "    k=5,\n",
    ")\n",
    "\n",
    "docs = retriever.invoke(\"What is RAG?\")\n",
    "print(f\"Retrieved {len(docs)} documents\")\n",
    "print(f\"Retrieved citations: {docs}\")\n",
    "for i, doc in enumerate(docs):\n",
    "    print(f\"\\n--- Document {i + 1} ---\")\n",
    "    print(f\"Content: {doc.page_content}...\")\n",
    "    print(f\"Score: {doc.metadata.get('score', 'N/A')}\")\n",
    "    print(f\"Source: {doc.metadata.get('document_name', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10e370d",
   "metadata": {},
   "source": [
    "## Customize Parameters\n",
    "\n",
    "`NvidiaRetriever` supports all DocumentSearch API parameters. Common options:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5b6725",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = NvidiaRetriever(\n",
    "    base_url=\"http://localhost:8081\",\n",
    "    collection_names=[\"test_multimodal_query\"],\n",
    "    k=4,                    # reranker_top_k: number of docs to return\n",
    "    vdb_top_k=50,           # candidates from vector DB before reranking\n",
    "    enable_reranker=False,\n",
    "    enable_query_rewriting=False,\n",
    ")\n",
    "\n",
    "docs = retriever.invoke(\"Explain approach 1\")\n",
    "print(f\"Retrieved {len(docs)} documents\")\n",
    "for i, doc in enumerate(docs):\n",
    "    print(f\"\\n--- Document {i + 1} ---\")\n",
    "    print(f\"Content: {doc.page_content}...\")\n",
    "    print(f\"Score: {doc.metadata.get('score', 'N/A')}\")\n",
    "    print(f\"Source: {doc.metadata.get('document_name', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f24a62",
   "metadata": {},
   "source": [
    "## Async Retrieval\n",
    "\n",
    "Use `ainvoke` for async workflows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf141913",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In Jupyter, use top-level await (don't use asyncio.run() - the kernel already has an event loop).\n",
    "retriever = NvidiaRetriever(\n",
    "    base_url=\"http://localhost:8081\",\n",
    "    collection_names=[\"test_multimodal_query\"],\n",
    "    k=3,\n",
    ")\n",
    "docs = await retriever.ainvoke(\"What is RAG?\")\n",
    "print(f\"Retrieved {len(docs)} documents\")\n",
    "for i, doc in enumerate(docs):\n",
    "    print(f\"\\n--- Document {i + 1} ---\")\n",
    "    print(f\"Content: {doc.page_content}...\")\n",
    "    print(f\"Score: {doc.metadata.get('score', 'N/A')}\")\n",
    "    print(f\"Source: {doc.metadata.get('document_name', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0392cc57",
   "metadata": {},
   "source": [
    "## Use in a RAG Chain\n",
    "\n",
    "Combine `NvidiaRetriever` with a chat model for end-to-end RAG. For `ChatNVIDIA` with the cloud API, set `NVIDIA_API_KEY` in your environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eee8fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA, NvidiaRetriever\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "retriever = NvidiaRetriever(\n",
    "    base_url=\"http://localhost:8081\",\n",
    "    collection_names=[\"test_multimodal_query\"],\n",
    "    k=4,\n",
    ")\n",
    "\n",
    "llm = ChatNVIDIA(model=\"meta/llama-3.1-8b-instruct\")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Answer based only on the following context:\\n\\n{context}\"),\n",
    "    (\"human\", \"{question}\"),\n",
    "])\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(d.page_content for d in docs)\n",
    "\n",
    "chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "response = chain.invoke(\"What is RAG?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f07c9f",
   "metadata": {},
   "source": [
    "## Error Handling\n",
    "\n",
    "The retriever raises specific exceptions when the RAG server is unreachable or returns errors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc17ca7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_nvidia_ai_endpoints import NvidiaRetriever\n",
    "from langchain_nvidia_ai_endpoints.retrievers import (\n",
    "    NvidiaRAGConnectionError,\n",
    "    NvidiaRAGServerError,\n",
    "    NvidiaRAGValidationError,\n",
    ")\n",
    "\n",
    "try:\n",
    "    retriever = NvidiaRetriever(\n",
    "        base_url=\"http://localhost:8081\",\n",
    "        collection_names=[\"test_multimodal_query\"],\n",
    "    )\n",
    "    docs = retriever.invoke(\"test query\")\n",
    "    print(f\"Success: {len(docs)} documents\")\n",
    "except NvidiaRAGConnectionError as e:\n",
    "    print(f\"Connection failed: {e}\")\n",
    "except NvidiaRAGValidationError as e:\n",
    "    print(f\"Validation error (422): {e}\")\n",
    "except NvidiaRAGServerError as e:\n",
    "    print(f\"Server error ({e.status_code}): {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612382a2",
   "metadata": {},
   "source": [
    "## Related Topics\n",
    "\n",
    "- [langchain-nvidia-ai-endpoints README](https://github.com/langchain-ai/langchain-nvidia/blob/main/libs/ai-endpoints/README.md)\n",
    "- [NVIDIA RAG Blueprint](https://github.com/NVIDIA/GenerativeAIExamples/tree/main/RAG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3890a066-ff0b-47dc-81e2-b972f1b179a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
