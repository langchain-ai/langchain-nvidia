{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro-title",
   "metadata": {},
   "source": [
    "# ChatNVIDIA with Dynamo KV Cache Optimization\n",
    "\n",
    "[NVIDIA Dynamo](https://developer.nvidia.com/dynamo) is an open-source, low-latency inference framework for serving generative AI models across GPU fleets. It includes four core components:\n",
    "\n",
    "- **Smart Router** — a KV cache-aware routing engine that uses Radix Tree data structures to track KV cache entries across GPUs. It computes overlap scores between incoming requests and cached KV blocks, routing each request to the worker that already holds the relevant cache — avoiding costly recomputation.\n",
    "- **GPU Resource Planner** — dynamically allocates resources between prefill and decode phases based on real-time capacity.\n",
    "- **Distributed KV Cache Manager** — manages KV cache across a memory hierarchy (GPU HBM, host DRAM, SSD, networked storage).\n",
    "- **NIXL** — a low-latency communication library for rapid KV cache movement between GPUs.\n",
    "\n",
    "`ChatNVIDIADynamo` is a drop-in replacement for `ChatNVIDIA` that automatically injects `nvext.agent_hints` into every request. These hints tell the Smart Router:\n",
    "\n",
    "- **`osl`** (output sequence length) — how many tokens to expect, so the scheduler can plan memory allocation\n",
    "- **`iat`** (inter-arrival time) — how quickly requests arrive, so the router can anticipate load\n",
    "- **`latency_sensitivity`** — how latency-critical a request is, so interactive calls get priority routing\n",
    "- **`priority`** — request priority, so background work can yield to critical-path requests\n",
    "\n",
    "A unique `prefix_id` is auto-generated for every request, enabling the router to track KV cache affinity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "i8euv0s4qt",
   "metadata": {},
   "source": [
    "## Development Setup\n",
    "\n",
    "If you are working from the `langchain-nvidia` repository, this project uses [Poetry](https://python-poetry.org/) to manage dependencies. Run the following from the `libs/ai-endpoints` directory:\n",
    "\n",
    "```bash\n",
    "cd libs/ai-endpoints\n",
    "pip install poetry                         # if not already installed\n",
    "poetry config virtualenvs.in-project true --local\n",
    "poetry install --with test\n",
    "```\n",
    "\n",
    "This creates a `.venv` inside `libs/ai-endpoints/`. Then install `ipykernel` directly via the venv's pip (not `poetry run`, which can recreate the environment) and register the Jupyter kernel:\n",
    "\n",
    "```bash\n",
    ".venv/bin/pip install ipykernel\n",
    ".venv/bin/python -m ipykernel install --user --name langchain-nvidia --display-name \"langchain-nvidia\"\n",
    "```\n",
    "\n",
    "After this, reload your editor window and select the **langchain-nvidia** kernel in the notebook kernel picker."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "install-header",
   "metadata": {},
   "source": [
    "## Install the Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "install-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade --quiet langchain-nvidia-ai-endpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prerequisites",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "This notebook targets a **local NIM deployment behind NVIDIA Dynamo**. Unlike the standard `ChatNVIDIA` workflow with the NVIDIA API Catalog, you do not need an `NVIDIA_API_KEY` — the NIM is running on your infrastructure.\n",
    "\n",
    "### Starting a Dynamo Deployment\n",
    "\n",
    "The fastest way to get started is with the [Dynamo Quickstart Guide](https://docs.nvidia.com/dynamo/latest/getting-started/quickstart). For more details, including Kubernetes deployment and multi-node setups, see the [Dynamo documentation](https://docs.nvidia.com/dynamo/latest/).\n",
    "\n",
    "> **Important:** When deploying the model, ensure the Dynamo worker's `--context-length` is at least **2x** the `MAX_TOKENS` value configured below. The context window must accommodate both the input prompt tokens and the completion tokens. For example, if `MAX_TOKENS = 4096`, deploy with `--context-length 8192` or higher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "config-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "NIM_BASE_URL = \"http://localhost:8099/v1\"\n",
    "MODEL = \"nvidia/NVIDIA-Nemotron-3-Nano-30B-A3B-BF16\"\n",
    "MAX_TOKENS = 4096"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "basic-usage-header",
   "metadata": {},
   "source": [
    "## Basic Usage\n",
    "\n",
    "`ChatNVIDIADynamo` accepts all the same parameters as `ChatNVIDIA`, plus four Dynamo-specific fields:\n",
    "\n",
    "| Parameter | Type | Default | Description |\n",
    "|-----------|------|---------|-------------|\n",
    "| `osl` | `int` | `512` | Expected output sequence length (tokens) |\n",
    "| `iat` | `int` | `250` | Expected inter-arrival time (ms) |\n",
    "| `latency_sensitivity` | `float` | `1.0` | Latency sensitivity hint (0.0 = tolerant, 1.0 = critical) |\n",
    "| `priority` | `int` | `1` | Request priority (higher = more important) |\n",
    "\n",
    "It is a drop-in replacement — swap `ChatNVIDIA` for `ChatNVIDIADynamo` and every request will automatically include routing hints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "basic-usage-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA, ChatNVIDIADynamo\n",
    "\n",
    "# Standard ChatNVIDIA — no Dynamo hints\n",
    "llm_standard = ChatNVIDIA(base_url=NIM_BASE_URL, model=MODEL, max_completion_tokens=MAX_TOKENS)\n",
    "\n",
    "# ChatNVIDIADynamo — identical interface, automatically injects agent_hints\n",
    "llm = ChatNVIDIADynamo(base_url=NIM_BASE_URL, model=MODEL, max_completion_tokens=MAX_TOKENS)\n",
    "\n",
    "result = llm.invoke(\"What is KV cache optimization?\")\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defaults-header",
   "metadata": {},
   "source": [
    "## Setting Defaults at Construction Time\n",
    "\n",
    "You can configure Dynamo hints when creating the model instance. This is useful when you know a model instance will always serve a particular role — e.g. a high-priority interactive assistant vs. a low-priority background summarizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defaults-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# High-priority: short responses, latency-critical\n",
    "llm_critical = ChatNVIDIADynamo(\n",
    "    base_url=NIM_BASE_URL,\n",
    "    model=MODEL,\n",
    "    max_completion_tokens=MAX_TOKENS,\n",
    "    osl=20,\n",
    "    priority=10,\n",
    "    latency_sensitivity=1.0,\n",
    ")\n",
    "\n",
    "# Low-priority: long responses, latency-tolerant\n",
    "llm_background = ChatNVIDIADynamo(\n",
    "    base_url=NIM_BASE_URL,\n",
    "    model=MODEL,\n",
    "    max_completion_tokens=MAX_TOKENS,\n",
    "    osl=512,\n",
    "    priority=1,\n",
    "    latency_sensitivity=0.1,\n",
    ")\n",
    "\n",
    "print(f\"Critical:   osl={llm_critical.osl}, priority={llm_critical.priority}, \"\n",
    "      f\"latency_sensitivity={llm_critical.latency_sensitivity}\")\n",
    "print(f\"Background: osl={llm_background.osl}, priority={llm_background.priority}, \"\n",
    "      f\"latency_sensitivity={llm_background.latency_sensitivity}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "overrides-header",
   "metadata": {},
   "source": [
    "## Per-Invocation Overrides\n",
    "\n",
    "Dynamo parameters can also be overridden on each call. This is useful when the same model instance handles requests with varying characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "overrides-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Override all four Dynamo parameters for a single request\n",
    "result = llm.invoke(\n",
    "    \"Classify this as positive or negative: 'I love this product!'\",\n",
    "    osl=10,\n",
    "    iat=100,\n",
    "    latency_sensitivity=1.0,\n",
    "    priority=10,\n",
    ")\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "streaming-header",
   "metadata": {},
   "source": [
    "## Streaming with Dynamo Hints\n",
    "\n",
    "Dynamo hints are included in the initial streaming request. The Smart Router uses them to select the optimal worker before tokens start flowing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "streaming-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk in llm_critical.stream(\"Give a one-sentence summary of GPU computing.\"):\n",
    "    print(chunk.content, end=\"\", flush=True)\n",
    "print()  # newline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chain-header",
   "metadata": {},
   "source": [
    "## LangChain Chain Integration\n",
    "\n",
    "`ChatNVIDIADynamo` works seamlessly in LangChain chains and pipelines, just like `ChatNVIDIA`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chain-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Classify the user's intent into exactly one category: \"\n",
    "               \"billing, technical_support, general_inquiry, or complaint. \"\n",
    "               \"Respond with only the category name.\"),\n",
    "    (\"user\", \"{input}\"),\n",
    "])\n",
    "\n",
    "classifier_chain = (\n",
    "    prompt\n",
    "    | ChatNVIDIADynamo(\n",
    "        base_url=NIM_BASE_URL,\n",
    "        model=MODEL,\n",
    "        max_completion_tokens=MAX_TOKENS,\n",
    "        osl=5,\n",
    "        priority=10,\n",
    "        latency_sensitivity=1.0,\n",
    "    )\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "intent = classifier_chain.invoke({\"input\": \"My invoice has the wrong amount\"})\n",
    "print(f\"Detected intent: {intent}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "agentic-intro",
   "metadata": {},
   "source": [
    "## Latency Sensitivity in Agentic Workflows\n",
    "\n",
    "The real power of `ChatNVIDIADynamo` emerges in multi-step agentic pipelines where **not all LLM calls are equally urgent**.\n",
    "\n",
    "Consider a customer support triage workflow. The first call (intent classification — ~5 output tokens) and the final call (quality review — ~20 tokens) are on the **critical path**: the user is actively waiting for a response. In between, several analysis branches run in **parallel** generating hundreds of tokens each. These background calls are important, but they have slack — they don't need to jump the queue.\n",
    "\n",
    "Without priority scheduling, all requests compete equally for GPU decode slots. When the GPU is saturated with long-running background decode requests, a short critical-path request (like a 5-token classification) gets queued behind them. With Dynamo's priority scheduling enabled (`--enable-priority-scheduling`), the short critical-path request **jumps the queue**, resulting in dramatically lower perceived latency.\n",
    "\n",
    "### Pipeline Design\n",
    "\n",
    "```\n",
    "                           ┌──────────────────────────┐\n",
    "                           │   Customer Query Input   │\n",
    "                           └────────────┬─────────────┘\n",
    "                                        │\n",
    "                           ┌────────────▼─────────────┐\n",
    "                           │  classify_query          │\n",
    "                           │  HIGH priority (10)      │\n",
    "                           │  osl=10, sensitivity=1.0 │\n",
    "                           └────────────┬─────────────┘\n",
    "                                        │\n",
    "          ┌──────────────┬──────────────┼──────────────┐\n",
    "          │              │              │              │\n",
    "  ┌───────▼───────┐ ┌───▼──────┐ ┌──────▼─────┐ ┌──────▼────────┐\n",
    "  │ research      │ │ lookup   │ │ check      │ │ analyze       │\n",
    "  │ _context      │ │ _policy  │ │ _compli-   │ │ _sentiment    │\n",
    "  │ LOW (1)       │ │ LOW (1)  │ │ ance       │ │ LOW (1)       │\n",
    "  │ osl=500       │ │ osl=500  │ │ LOW (1)    │ │ osl=500       │\n",
    "  │ sens=0.1      │ │ sens=0.1 │ │ osl=500    │ │ sens=0.1      │\n",
    "  └───────┬───────┘ └───┬──────┘ │ sens=0.1   │ └────┬──────────┘\n",
    "          │             │        └─ ───┬──────┘      │\n",
    "          └─────────────┴──────────────┼─────────────┘\n",
    "                                       │\n",
    "                          ┌────────────▼─────────────┐\n",
    "                          │  draft_response          │\n",
    "                          │  MED priority (5)        │\n",
    "                          │  osl=500, sensitivity=0.5│\n",
    "                          └────────────┬─────────────┘\n",
    "                                       │\n",
    "                          ┌────────────▼─────────────┐\n",
    "                          │  review_response         │\n",
    "                          │  HIGH priority (10)      │\n",
    "                          │  osl=20, sensitivity=1.0 │\n",
    "                          └──────────────────────────┘\n",
    "```\n",
    "\n",
    "| Node | Priority | `osl` | Role |\n",
    "|------|----------|-------|------|\n",
    "| `classify_query` | HIGH (10) | 10 | Entry point — all downstream nodes depend on it |\n",
    "| `research_context` | LOW (1) | 500 | Parallel background — has slack |\n",
    "| `lookup_policy` | LOW (1) | 500 | Parallel background — has slack |\n",
    "| `check_compliance` | LOW (1) | 500 | Parallel background — has slack |\n",
    "| `analyze_sentiment` | LOW (1) | 500 | Parallel background — has slack |\n",
    "| `draft_response` | MED (5) | 500 | Join point — synthesizes all branches |\n",
    "| `review_response` | HIGH (10) | 20 | Exit point — user is waiting for this |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pipeline-models-header",
   "metadata": {},
   "source": [
    "### Define Model Instances & Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pipeline-models-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "from langchain_nvidia_ai_endpoints import ChatNVIDIADynamo\n",
    "\n",
    "# High-priority model: critical-path calls (classification, review)\n",
    "llm_high = ChatNVIDIADynamo(\n",
    "    base_url=NIM_BASE_URL,\n",
    "    model=MODEL,\n",
    "    max_completion_tokens=MAX_TOKENS,\n",
    "    priority=10,\n",
    "    latency_sensitivity=1.0,\n",
    ")\n",
    "\n",
    "# Medium-priority model: join point (draft)\n",
    "llm_med = ChatNVIDIADynamo(\n",
    "    base_url=NIM_BASE_URL,\n",
    "    model=MODEL,\n",
    "    max_completion_tokens=MAX_TOKENS,\n",
    "    priority=5,\n",
    "    latency_sensitivity=0.5,\n",
    ")\n",
    "\n",
    "# Low-priority model: parallel background analysis\n",
    "llm_low = ChatNVIDIADynamo(\n",
    "    base_url=NIM_BASE_URL,\n",
    "    model=MODEL,\n",
    "    max_completion_tokens=MAX_TOKENS,\n",
    "    priority=1,\n",
    "    latency_sensitivity=0.1,\n",
    ")\n",
    "\n",
    "# --- Stage 1: Intent Classification (HIGH) ---\n",
    "classify_chain = (\n",
    "    ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"Classify the customer query into one of: billing, \"\n",
    "                   \"technical_support, account, or general. Respond with \"\n",
    "                   \"only the category.\"),\n",
    "        (\"user\", \"{query}\"),\n",
    "    ])\n",
    "    | llm_high.bind(osl=10)\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# --- Stage 2a: Research Context (LOW) ---\n",
    "research_chain = (\n",
    "    ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"Research relevant context for this customer query. Provide \"\n",
    "                   \"background information, common causes, and any relevant \"\n",
    "                   \"product details that would help a support agent.\"),\n",
    "        (\"user\", \"Category: {category}\\nQuery: {query}\"),\n",
    "    ])\n",
    "    | llm_low.bind(osl=500)\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# --- Stage 2b: Lookup Policy (LOW) ---\n",
    "policy_chain = (\n",
    "    ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"Based on this customer query, identify the relevant company \"\n",
    "                   \"policies. Include refund policies, SLAs, escalation \"\n",
    "                   \"procedures, and any applicable customer guarantees.\"),\n",
    "        (\"user\", \"Category: {category}\\nQuery: {query}\"),\n",
    "    ])\n",
    "    | llm_low.bind(osl=500)\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# --- Stage 2c: Check Compliance (LOW) ---\n",
    "compliance_chain = (\n",
    "    ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"Review this customer interaction for compliance considerations. \"\n",
    "                   \"Flag any regulatory requirements, data privacy concerns, \"\n",
    "                   \"or mandatory disclosures that must be included in the response.\"),\n",
    "        (\"user\", \"Category: {category}\\nQuery: {query}\"),\n",
    "    ])\n",
    "    | llm_low.bind(osl=500)\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# --- Stage 2d: Analyze Sentiment (LOW) ---\n",
    "sentiment_chain = (\n",
    "    ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"Analyze the sentiment and emotional tone of this customer \"\n",
    "                   \"message. Identify the level of urgency, frustration, and \"\n",
    "                   \"any specific emotional cues that should inform the response tone.\"),\n",
    "        (\"user\", \"{query}\"),\n",
    "    ])\n",
    "    | llm_low.bind(osl=500)\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# --- Stage 3: Draft Response (MED) ---\n",
    "draft_chain = (\n",
    "    ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"You are a customer support agent. Using the analysis below, \"\n",
    "                   \"draft a helpful and empathetic response to the customer.\\n\\n\"\n",
    "                   \"Category: {category}\\n\"\n",
    "                   \"Sentiment: {sentiment}\\n\"\n",
    "                   \"Context: {context}\\n\"\n",
    "                   \"Policy: {policy}\\n\"\n",
    "                   \"Compliance: {compliance}\"),\n",
    "        (\"user\", \"{query}\"),\n",
    "    ])\n",
    "    | llm_med.bind(osl=500)\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# --- Stage 4: Review Response (HIGH) ---\n",
    "review_chain = (\n",
    "    ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"Review this draft customer support response. Reply with only \"\n",
    "                   \"APPROVED if it is ready to send, or NEEDS_REVISION followed \"\n",
    "                   \"by a brief reason.\"),\n",
    "        (\"user\", \"Draft response:\\n{draft}\"),\n",
    "    ])\n",
    "    | llm_high.bind(osl=20)\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "run-pipeline-header",
   "metadata": {},
   "source": [
    "### Run the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run-pipeline-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "\n",
    "async def triage_customer_query(query: str) -> dict:\n",
    "    \"\"\"Run the full triage pipeline with timing.\"\"\"\n",
    "\n",
    "    # Stage 1: Classification (critical path — HIGH priority)\n",
    "    t0 = time.time()\n",
    "    category = await classify_chain.ainvoke({\"query\": query})\n",
    "    category = category.strip()\n",
    "    t1 = time.time()\n",
    "    print(f\"  classify_query:    {t1 - t0:.2f}s  [HIGH]  -> {category}\")\n",
    "\n",
    "    # Stage 2: Parallel background analysis (LOW priority)\n",
    "    t2 = time.time()\n",
    "    context, policy, compliance, sentiment = await asyncio.gather(\n",
    "        research_chain.ainvoke({\"query\": query, \"category\": category}),\n",
    "        policy_chain.ainvoke({\"query\": query, \"category\": category}),\n",
    "        compliance_chain.ainvoke({\"query\": query, \"category\": category}),\n",
    "        sentiment_chain.ainvoke({\"query\": query}),\n",
    "    )\n",
    "    t3 = time.time()\n",
    "    print(f\"  parallel_analysis: {t3 - t2:.2f}s  [LOW]   (4 branches)\")\n",
    "\n",
    "    # Stage 3: Draft response (MED priority)\n",
    "    t4 = time.time()\n",
    "    draft = await draft_chain.ainvoke({\n",
    "        \"query\": query,\n",
    "        \"category\": category,\n",
    "        \"sentiment\": sentiment,\n",
    "        \"context\": context,\n",
    "        \"policy\": policy,\n",
    "        \"compliance\": compliance,\n",
    "    })\n",
    "    t5 = time.time()\n",
    "    print(f\"  draft_response:    {t5 - t4:.2f}s  [MED]\")\n",
    "\n",
    "    # Stage 4: Review (critical path — HIGH priority)\n",
    "    t6 = time.time()\n",
    "    review = await review_chain.ainvoke({\"draft\": draft})\n",
    "    t7 = time.time()\n",
    "    print(f\"  review_response:   {t7 - t6:.2f}s  [HIGH]  -> {review.strip()[:40]}\")\n",
    "\n",
    "    return {\n",
    "        \"category\": category,\n",
    "        \"sentiment\": sentiment,\n",
    "        \"context\": context,\n",
    "        \"policy\": policy,\n",
    "        \"compliance\": compliance,\n",
    "        \"draft\": draft,\n",
    "        \"review\": review,\n",
    "        \"timing\": {\n",
    "            \"classify\": t1 - t0,\n",
    "            \"parallel_analysis\": t3 - t2,\n",
    "            \"draft\": t5 - t4,\n",
    "            \"review\": t7 - t6,\n",
    "            \"total\": t7 - t0,\n",
    "            \"critical_path\": (t1 - t0) + (t7 - t6),\n",
    "        },\n",
    "    }\n",
    "\n",
    "\n",
    "customer_query = (\n",
    "    \"I've been a loyal customer for 5 years, but my last three orders \"\n",
    "    \"(#ORD-8821, #ORD-8834, #ORD-8901) have all arrived damaged. \"\n",
    "    \"I was charged $149.99 for the most recent one on Jan 15th and \"\n",
    "    \"still haven't received a refund. Your support chat was down \"\n",
    "    \"yesterday when I tried to reach out. I'm very frustrated.\"\n",
    ")\n",
    "\n",
    "print(\"Running triage pipeline...\\n\")\n",
    "results = await triage_customer_query(customer_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "results-header",
   "metadata": {},
   "source": [
    "### Display Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "results-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML, display\n",
    "\n",
    "t = results[\"timing\"]\n",
    "\n",
    "html = f\"\"\"\n",
    "<h3>Triage Results</h3>\n",
    "\n",
    "<table style=\"border-collapse:collapse; width:100%; margin-bottom:20px;\">\n",
    "  <tr>\n",
    "    <td style=\"padding:8px; font-weight:bold; width:140px;\">Category</td>\n",
    "    <td style=\"padding:8px;\">{results['category'].split('\\\\n')[-1].strip()}</td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "<details style=\"margin-bottom:12px;\">\n",
    "  <summary style=\"cursor:pointer; font-weight:bold;\">Sentiment Analysis</summary>\n",
    "  <pre style=\"white-space:pre-wrap; background:#f8f8f8; padding:12px; margin-top:8px; border-radius:4px;\">{results['sentiment'][:500]}</pre>\n",
    "</details>\n",
    "\n",
    "<details style=\"margin-bottom:12px;\">\n",
    "  <summary style=\"cursor:pointer; font-weight:bold;\">Draft Response</summary>\n",
    "  <pre style=\"white-space:pre-wrap; background:#f8f8f8; padding:12px; margin-top:8px; border-radius:4px;\">{results['draft'][:800]}</pre>\n",
    "</details>\n",
    "\n",
    "<details style=\"margin-bottom:12px;\">\n",
    "  <summary style=\"cursor:pointer; font-weight:bold;\">Review Verdict</summary>\n",
    "  <pre style=\"white-space:pre-wrap; background:#f8f8f8; padding:12px; margin-top:8px; border-radius:4px;\">{results['review'].split('\\\\n')[-1].strip()}</pre>\n",
    "</details>\n",
    "\n",
    "<h3>Timing Breakdown</h3>\n",
    "\n",
    "<table style=\"border-collapse:collapse; width:100%;\">\n",
    "  <thead>\n",
    "    <tr style=\"border-bottom:2px solid #333;\">\n",
    "      <th style=\"text-align:left; padding:8px;\">Stage</th>\n",
    "      <th style=\"text-align:center; padding:8px;\">Priority</th>\n",
    "      <th style=\"text-align:right; padding:8px;\">Time</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr style=\"background-color:#e8f5e9;\">\n",
    "      <td style=\"padding:8px;\">classify_query</td>\n",
    "      <td style=\"text-align:center; padding:8px;\"><strong>HIGH</strong></td>\n",
    "      <td style=\"text-align:right; padding:8px;\">{t['classify']:.2f}s</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td style=\"padding:8px;\">parallel_analysis (4 branches)</td>\n",
    "      <td style=\"text-align:center; padding:8px;\">LOW</td>\n",
    "      <td style=\"text-align:right; padding:8px;\">{t['parallel_analysis']:.2f}s</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td style=\"padding:8px;\">draft_response</td>\n",
    "      <td style=\"text-align:center; padding:8px;\">MED</td>\n",
    "      <td style=\"text-align:right; padding:8px;\">{t['draft']:.2f}s</td>\n",
    "    </tr>\n",
    "    <tr style=\"background-color:#e8f5e9;\">\n",
    "      <td style=\"padding:8px;\">review_response</td>\n",
    "      <td style=\"text-align:center; padding:8px;\"><strong>HIGH</strong></td>\n",
    "      <td style=\"text-align:right; padding:8px;\">{t['review']:.2f}s</td>\n",
    "    </tr>\n",
    "    <tr style=\"border-top:2px solid #333; background-color:#e8f5e9;\">\n",
    "      <td style=\"padding:8px;\" colspan=\"2\"><strong>Critical path</strong> (classify + review)</td>\n",
    "      <td style=\"text-align:right; padding:8px;\"><strong>{t['critical_path']:.2f}s</strong></td>\n",
    "    </tr>\n",
    "    <tr style=\"border-top:1px solid #ccc;\">\n",
    "      <td style=\"padding:8px;\" colspan=\"2\"><strong>Total wall clock</strong></td>\n",
    "      <td style=\"text-align:right; padding:8px;\"><strong>{t['total']:.2f}s</strong></td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "\"\"\"\n",
    "\n",
    "display(HTML(html))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "under-the-hood",
   "metadata": {},
   "source": [
    "## How It Works Under the Hood\n",
    "\n",
    "When `ChatNVIDIADynamo` sends a request, it injects an `nvext.agent_hints` section into the request payload. Here is what the payloads look like for our high-priority and low-priority calls:\n",
    "\n",
    "**High-priority request** (classification / review — critical path):\n",
    "```json\n",
    "{\n",
    "  \"model\": \"meta/llama-3.1-8b-instruct\",\n",
    "  \"messages\": [{\"role\": \"user\", \"content\": \"...\"}],\n",
    "  \"nvext\": {\n",
    "    \"agent_hints\": {\n",
    "      \"prefix_id\": \"langchain-dynamo-a1b2c3d4e5f6\",\n",
    "      \"osl\": 10,\n",
    "      \"iat\": 250,\n",
    "      \"latency_sensitivity\": 1.0,\n",
    "      \"priority\": 10\n",
    "    }\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "**Low-priority request** (parallel background analysis):\n",
    "```json\n",
    "{\n",
    "  \"model\": \"meta/llama-3.1-8b-instruct\",\n",
    "  \"messages\": [{\"role\": \"user\", \"content\": \"...\"}],\n",
    "  \"nvext\": {\n",
    "    \"agent_hints\": {\n",
    "      \"prefix_id\": \"langchain-dynamo-f6e5d4c3b2a1\",\n",
    "      \"osl\": 500,\n",
    "      \"iat\": 250,\n",
    "      \"latency_sensitivity\": 0.1,\n",
    "      \"priority\": 1\n",
    "    }\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "The Smart Router uses these hints to make scheduling decisions:\n",
    "\n",
    "- **`prefix_id`** — auto-generated per request (`langchain-dynamo-<uuid>`), enabling the router to track KV cache entries\n",
    "- **`osl`** — pre-allocate the right amount of GPU memory for output tokens\n",
    "- **`iat`** — predict incoming load for capacity planning\n",
    "- **`latency_sensitivity`** — decide whether to queue or fast-track the request\n",
    "- **`priority`** — determine scheduling order when requests compete for GPU resources\n",
    "\n",
    "When the Dynamo worker is started with `--enable-priority-scheduling`, requests with higher priority values are scheduled ahead of lower-priority ones, even if the lower-priority requests arrived first. This means the 4 parallel background branches (~500 tokens each) yield GPU decode time to the short classification and review calls (~10-20 tokens), reducing perceived latency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inspect-header",
   "metadata": {},
   "source": [
    "## Inspecting the Payload\n",
    "\n",
    "For debugging, you can inspect the exact payload that will be sent to the NIM endpoint using the internal `_get_payload` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inspect-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "payload = llm_critical._get_payload(\n",
    "    inputs=[{\"role\": \"user\", \"content\": \"Hello!\"}],\n",
    "    stop=None,\n",
    ")\n",
    "\n",
    "# Show the nvext section with agent_hints\n",
    "print(json.dumps(payload[\"nvext\"], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "<table>\n",
    "  <thead>\n",
    "    <tr>\n",
    "      <th style=\"text-align:left\">Feature</th>\n",
    "      <th style=\"text-align:center\"><code>ChatNVIDIA</code></th>\n",
    "      <th style=\"text-align:center\"><code>ChatNVIDIADynamo</code></th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr><td>API Catalog / NIM support</td><td style=\"text-align:center\">Yes</td><td style=\"text-align:center\">Yes</td></tr>\n",
    "    <tr><td>Streaming</td><td style=\"text-align:center\">Yes</td><td style=\"text-align:center\">Yes</td></tr>\n",
    "    <tr><td>Tool calling</td><td style=\"text-align:center\">Yes</td><td style=\"text-align:center\">Yes</td></tr>\n",
    "    <tr><td>Structured output</td><td style=\"text-align:center\">Yes</td><td style=\"text-align:center\">Yes</td></tr>\n",
    "    <tr><td>LangChain chains & agents</td><td style=\"text-align:center\">Yes</td><td style=\"text-align:center\">Yes</td></tr>\n",
    "    <tr style=\"background-color:#f0f7f0\"><td><strong>KV cache routing hints</strong> (<code>nvext.agent_hints</code>)</td><td style=\"text-align:center\">&mdash;</td><td style=\"text-align:center\"><strong>Yes</strong></td></tr>\n",
    "    <tr style=\"background-color:#f0f7f0\"><td><strong>Per-request <code>osl</code> / <code>iat</code></strong></td><td style=\"text-align:center\">&mdash;</td><td style=\"text-align:center\"><strong>Yes</strong></td></tr>\n",
    "    <tr style=\"background-color:#f0f7f0\"><td><strong>Priority-based scheduling</strong></td><td style=\"text-align:center\">&mdash;</td><td style=\"text-align:center\"><strong>Yes</strong></td></tr>\n",
    "    <tr style=\"background-color:#f0f7f0\"><td><strong>Latency sensitivity hints</strong></td><td style=\"text-align:center\">&mdash;</td><td style=\"text-align:center\"><strong>Yes</strong></td></tr>\n",
    "    <tr style=\"background-color:#f0f7f0\"><td><strong>Auto-generated <code>prefix_id</code></strong></td><td style=\"text-align:center\">&mdash;</td><td style=\"text-align:center\"><strong>Yes</strong></td></tr>\n",
    "  </tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "related-topics",
   "metadata": {},
   "source": [
    "## Related Topics\n",
    "\n",
    "- [NVIDIA Dynamo](https://developer.nvidia.com/dynamo) — open-source inference framework\n",
    "- [Dynamo Quickstart Guide](https://docs.nvidia.com/dynamo/latest/getting-started/quickstart) — get a local deployment running\n",
    "- [KV Cache-Aware Routing](https://docs.nvidia.com/dynamo/latest/user-guides/kv-cache-aware-routing) — how the Smart Router works\n",
    "- [AIQ Toolkit Latency Sensitivity Demo](https://github.com/NVIDIA/AIQToolkit/tree/main/examples/dynamo_integration/latency_sensitivity_demo) — extended example with profiling\n",
    "- [ChatNVIDIA Documentation](nvidia_ai_endpoints.ipynb) — standard ChatNVIDIA usage\n",
    "- [langchain-nvidia-ai-endpoints README](https://github.com/langchain-ai/langchain-nvidia/blob/main/libs/ai-endpoints/README.md)\n",
    "- [NVIDIA NIM Microservices](https://www.nvidia.com/en-us/ai-data-science/products/nim-microservices/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-nvidia-ai-endpoints-py3.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
